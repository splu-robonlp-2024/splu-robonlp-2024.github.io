<!DOCTYPE html>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
<title>SpLU-RoboNLP 2024</title>
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link
    href='http://fonts.googleapis.com/css?family=Open+Sans:400,700'
rel='stylesheet' type='text/css'>
<link rel="stylesheet"
    href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css"
    integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7"
crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css"
href="stylesheets/github-light.css" media="screen">

<!-- Latest compiled and minified CSS -->


  </head>
  <body>
<section class="page-header">

      <h1 class="project-name"><b>SpLU-RoboNLP 2024</b></h1>
      <h2 class="project-tagline">Fourth International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics</h2>
      <h2 class="project-tagline">In conjunction with The 62nd Annual Meeting of the Association for Computational Linguistics (ACL2024), August 16, 2024, Bangkok, Thailand.</h2>
      <a href="https://2024.aclweb.org/" style="color:white;">(Proceedings)</a> 
<!--      <h2  class="project-date">August 6th, 2021</h2>-->
	  </h2>
<br>

</section>
<section>
  <!-- Static navbar -->
      <nav class="navbar navbar-default">
        <div class="container-fluid">
          <div class="navbar-header">
            <butfton type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>

          </div>
          <div id="navbar" class="navbar-collapse collapse">
            <ul class="nav navbar-nav">
              <li><a href="#topics">Topics</a></li>
              <li><a href="#cfp">CFP</a></li>
              <li><a href="#submission-info">Submission</a></li>
              <li><a href="#invitedSpeakers">Speakers</a></li>
              <!--<li><a href="#papers">Papers</a></li>-->
              <li><a href="#schedule">Schedule</a></li>
              <li><a href="#organizers">Organizers</a></li>
              <li><a href="#program-commitee">Program Committee</a></li>
	      <li> <div class="dropdown">
                <button class="dropbtn">Past SpLU-RoboNLP</button>
                <div class="dropdown-content">
                  <a href="https://splu-robonlp-2023.github.io/">SpLU-RoboNLP @ EMNLP 2023</a>
                  <a href="https://splu-robonlp2021.github.io/">SpLU-RoboNLP @ ACL 2021</a>
		  <a href="https://spatial-language.github.io/">SpLU @ EMNLP 2020</a>
                  <a href="https://splu-robonlp.github.io/">SpLU-RoboNLP @ NAACL 2019</a>
                  <a href="https://spatial-language.github.io/old_SpLU_workshops/SpLU_2018/">SpLU @ NAACL 2018</a>
                  <a href="https://robo-nlp.github.io/2017_index.html">RoboNLP @ ACL 2017</a>
                </div>
              </div>
            </li>
         </ul>
            </div><!--/.nav-collapse -->
        </div><!--/.container-fluid -->
      </nav>

</section>


<section class="main-content">

<h2>Aim and Scope</h2>

Leveraging the foundation built in the prior workshops SpLU-RoboNLP 2023, SpLU-RoboNLP 2021, SpLU 2020, SpLU-RoboNLP 2019, SpLU 2018, and RoboNLP 2017, we propose the fourth combined workshop on Spatial Language Understanding and Grounded Communication for Robotics. Natural language communication with general-purpose embodied robots has long been a dream inspired by science fiction, and natural language interfaces have the potential to make robots more accessible to a wider range of users. Achieving this goal requires the continuous improvement of and development of new technologies for linking language to perception and action in the physical world. In particular, given the rise of large vision and language generative models, spatial language understanding and natural interactions have become more exciting topics to explore. This joint workshop aims to bring together the perspectives of researchers working on physical robot systems with human users, simulated embodied environments, multimodal interaction, and spatial language understanding to forge collaborations.


<a id="topics" class="anchor" href="#topics" aria-hidden="true"><span class="octicon octicon-link"></span></a>
<h2>Topics of Interest</h2>
<hr/>
We are interested in but not limited to original research in developing computational models, benchmarks, evaluation metrics, analysis, surveys and position papers on the following topics:
<ol>
<li>Deployment of Large Language Models for Situated Dialogue and Language Grounding</li>
<li>Spatial Reasoning with Large Language Models</li>
<li>Aligning and Translating Language to Situated Actions</li>
<li>Evaluation Metrics for Language Grounding and Human-Robot Communication</li>
<li>Human-Computer Interactions Through Natural or Structural Language</li>
<li>Instruction Understanding and Spatial Reasoning based on Multimodal Information for Navigation, Articulation, and Manipulation</li>
<li>Interactive Situated Dialogue for Physical Tasks</li>
<li>Language-based Game Playing for Grounding</li>
<li>Spatial Language and Skill Learning via Grounded Dialogue</li>
<li>(Spatial) Language Generation for Embodied Tasks</li>
<li>(Spatially-) Grounded Knowledge Representations</li>
<li>Spatial Reasoning in Image and Video Diffusion Models</li>
<li>Qualitative Spatial Representations and Neuro-symbolic Modeling</li>
<li>Utilization and Limitations of Large (Multimodal-)Language Models in Spatial Understanding and Grounded Communication</li>
</ol>


<a id="cfp" class="anchor" href="#cfp" aria-hidden="true"><span class="octicon octicon-link"></span></a>
<h2>Call for Papers</h2><hr/>
We cordially invite authors to contribute by submitting their long papers and short papers. 

<h4> Long Papers </h4>
Technical papers: 8 pages excluding references, 1 additional page allowed for camera reaedy. <br/>

<h4> Short Papers </h4>
Position statements describing previously unpublished work or demos: 4 pages excluding references, 1 additional page allowed for camera reaedy. <br/>

<h4>Non-Archival Option</h4> 
ACL workshops are traditionally archival. To allow dual submission of your work to SpLU-RoboNLP 2024 from *ACL Findings and other conferences/journals, we are also including a non-archival track. Space permitting, these submissions will still participate and present their work in the workshop, and will be hosted on the workshop website, but will not be included in the official proceedings. Please apply the ACL format and submit through OpenReview, but indicate that this is a cross-submission (non-archival) at the bottom of the submission form.


<a id="submission-info" class="anchor" href="#submission-info" aria-hidden="true"><span class="octicon octicon-link"></span></a>
<h2>Submission Instructions</h2><hr/>
All submissions must be in compliance with the ACL formatting guidelines and code of ethics.
Papers should be submitted electronically through the OpenReview protocol.
The peer review will be double-blind.

<h4>Style and Formatting</h4> 
<a href="https://github.com/acl-org/ACLPUB/tree/master/templates/">ACL Template</a> 
<br/>

<h4>Submissions Website</h4> 
<a href="https://openreview.net/group?id=aclweb.org/ACL/2024/Workshop/SpLU-RoboNLP">OpenReview</a> 
<br/>

<h4>Important Dates</h4>
<ul>
 <li>Submission Open: <b>5 Feburary 2024</b> (Anywhere on Earth) </li>
  <li><s>Submission Deadline: <b>17 May 2024</b> (Anywhere on Earth)</s> </li>
  <li><FONT COLOR=red> Extended Submission Deadline: <b>27 May 2024</b> (Anywhere on Earth)</FONT></li>
  <li><s>Notification of Acceptance:<b>17 June 2024</b> (Anywhere on Earth)</s></li>
  <li><FONT COLOR=red> Extended Notification of Acceptance:<b>24 June 2024</b> (Anywhere on Earth)</FONT></li>
  <li>Camera Ready Deadline: <b>30 June 2024</b> (GMT) </li>
  <li>Workshop Day: <b>16 August 2024</b> (co-located with ACL 2024)</li>
</ul> 


<a id="invitedSpeakers" class="anchor" href="#invitedSpeakers" aria-hidden="true"><span class="octicon octicon-link"></span></a>
<h2>Invited Speakers</h2>
<hr/>

<table width="80%" border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center"><img class="image1" src="images/inderjeet.jpeg" width="500" height="300" /> </td>
    <td align="left" width="80%"><a href="https://sites.google.com/site/inderjeetmani/">Inderjeet Mani</a> (<i>Formerly, Yahoo! Labs, Georgetown University</i>)
    <br><b>Title</b>: Grounding Spatial Natural Language with Generative AI
    <br><b>Bio</b>: Inderjeet Mani is a research scientist specializing in NLP and AI. His research in NLP has included areas like automatic summarization, narrative modeling, and temporal and spatial information extraction from text. He has also contributed to research on question-answering, bioinformatics, ontologies, machine translation, geographical information systems, and multimedia information processing. His publications include a hundred-odd scientific papers (totaling over 13,000 citations), along with six scholarly books and a science-fiction thriller, along with nearly fifty shorter literary pieces, including a popular essay about robot readers. Currently semi-retired, his prior affiliations have included Georgetown University (Associate Professor), Yahoo (Senior Director), Cambridge University (Visiting Fellow), MITRE (Senior Principal Scientist), Brandeis University (Visiting Scholar), MIT (Research Affiliate), and the Indian Institute of Science (Consulting Scientist). He has also served on the editorial boards of the journals Computational Linguistics (2002-2004) and Natural Language Engineering (2011-2015), while also reviewing for numerous AI journals and conferences.
    <br><b>Abstract</b>: When communicating in NL about spatial relations and movement, humans seldom use precise geometries and equations of motion, instead relying on a qualitative understanding. Qualitative and quantitative grounding of spatial relations in NL can together provide more flexible and higher-level communication with robots. I begin with an outline of a spatial semantics for natural language based on qualitative reasoning formalisms. Generative AI systems can extract spatial relations based on these formalisms from visual and/or text data, and generate NL descriptions based on them. They also appear to carry out rudimentary reasoning using these formalisms, providing large-scale, albeit noisy, training data. However, for the results to be convincing, formal reasoning needs to be integrated more tightly with neural architectures, beginning with improved tokenization. Evaluations need to go well beyond SOTA benchmarks and must extend to extrinsic tasks of interest to robotics.
    </br></td> 
  </tr>
  <tr>
    <td align="center"><img class="image1" src="images/Malihe-Alikhani.jpg" width="300" height="300" /></td>
    <td align="left" width="80%"><a href="https://alikhanimalihe.wixsite.com/mysite">Malihe Alikhani </a> (<i>Northeastern University</i>)
    <br><b>Title</b>: From Ambiguity to Clarity: Navigating Uncertainty in Human-Machine Conversations
    <br><b>Bio</b>: Malihe Alikhani is an assistant professor of AI and social justice at the Khoury School of Computer Science, Northeastern University. She is affiliated with the Northeastern Ethics Institute as well of the Institute for Experiential AI. Her research interests center on using representations of communicative structure, machine learning, and cognitive science to design equitable and inclusive NLP systems for critical applications such as education, health, and social justice. She has designed several models for sign language understanding generation, dialogue systems for deaf and hard of hearing and AI systems for evaluation of speech impairment. She has worked on projects with fair and limited data. Her work has received multiple best paper awards at ACL 2021, UAI2022, INLG2021, UMAP2022, and EMNLP 2023  and has been supported by DARPA, NIH, CDC, Google, and Amazon.
    <br><b>Abstract</b>: This talk delves into the intricacies of uncertainty in human-machine dialogue, mainly focusing on the challenges and solutions related to ambiguities arising from impoverished contextual representations. We examine how linguistically informed context representations can mitigate data-related uncertainty in a deployed dialogue system similar to Alexa. We acknowledge that certain types of data-related uncertainty are unavoidable and investigate the capabilities of modern billion-scale language models in representing this form of uncertainty in conversations. Shifting our focus to epistemic uncertainty arising from misaligned background knowledge between humans and machines, we explore strategies for quantifying and reducing this form of uncertainty. Our discussion encompasses various facets of human-machine convergence, including Theory of Mind, fairness, and pragmatics. By leveraging machine learning theory and cognitive science insights, we aim to quantify epistemic uncertainty and propose algorithms that improve grounding between humans and machines. This exploration sheds light on the theoretical underpinnings of uncertainty in dialogue systems and offers practical solutions for improving human-machine communication.
    </br></td>
  </tr>
  <tr>
    <td align="center"><img class="image1" src="images/daniel.jpg" width="300" height="300" /></td>
    <td align="left" width="80%"><a href="https://dpfried.github.io/">Daniel Fried</a>(<i>Carnegie Mellon University</i>)
     <br><b>Title</b>: TBU
    <br><b>Bio</b>: TBU
    <br><b>Abstract</b>:TBU
    </br></td>
  </tr>
    <tr>
    <td align="center"><img class="image1" src="images/yu.jpg" width="300" height="300" /></td>
    <td align="left" width="80%"><a href="https://ysu1989.github.io/">Yu Su</a> (<i>Ohio State University</i>)
     <br><b>Title</b>: Web Agents: A New Frontier for Embodied Agents
    <br><b>Bio</b>: Yu Su is a Distinguished Assistant Professor at the Ohio State University and a researcher at Microsoft. He co-directs the OSU NLP group. He has broad interests in artificial and biological intelligence, with a primary interest in the role of language as a vehicle of thought and communication. His recent interests center around language agents with a series of well-received works such as Mind2Web, SeeAct, HippoRAG, LLM-Planner, and MMMU. His work has received multiple paper awards, including the Best Student Paper Award at CVPR 2024 and Outstanding Paper Award at ACL 2023.
    <br><b>Abstract</b>:The digital world, e.g., the World Wide Web, provides a powerful yet underexplored form of embodiment for AI agents, where perception involves understanding visual renderings and the underlying markup languages, and effectors are mice and keyboards. Empowered by large language models (LLMs), web agents are rapidly rising as a new frontier for embodied agents that provide both the breadth and depth needed for driving agent development. On the other hand, web agents can potentially lead to many practical applications, thus raising substantial commercial interests as well. In this talk, I will present an overview of web agents, covering the history, the promises, and the challenges. I will then give an in-depth discussion on multimodality and grounding. Finally, I will conclude with an outlook of promising future directions, including planning, synthetic data, and safety.
    </br></td>
  </tr>
  <tr>
    <td align="center"><img class="image1" src="images/yoav.jpg" width="300" height="300" /></td>
    <td align="left" width="80%"><a href="https://yoavartzi.com/">Yoav Artzi</a> (<i>Cornell University</i>)
    <br><b>Title</b>: TBU
    <br><b>Bio</b>: TBU
    <br><b>Abstract</b>:TBU
    </br></td>
  </tr>
  <tr>
    <td align="center"><img class="image1" src="images/maning.jpg" width="300" height="300" /></td>
    <td align="left" width="80%"><a href="https://limanling.github.io/">Manling Li</a> (<i>Northwestern University</i>)
    <br><b>Title</b>: TBU
    <br><b>Bio</b>: TBU
    <br><b>Abstract</b>:TBU
    </br></td>
  </tr>

</table>

<!-- 
<div class="speaker-block">
  <font size="3" color="black"><b><a href="https://www.cs.princeton.edu/~karthikn/">Karthik Narasimhan</a>, Princeton University</b></font>
  
  <p><b>TITLE</b></p>
  <p><b>Abstract:</b>
    ABSTRACT
  </p>
  <p><b>Bio:</b>
    BIO
  </p>
</div>
-->

<!-- 
<a id="papers" class="anchor" href="#papers" aria-hidden="true"><span class="octicon octicon-link"></span></a>
<h2>Accepted Papers</h2> 
-->
  

<a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span class="octicon octicon-link"></span></a>
<h2>Schedule</h2>

<iframe id="schedule-iframe" 
  src="https://docs.google.com/document/d/e/2PACX-1vS-S_GoL6uf3LwTyBZRc9X5pZl6MoyoL-eLTtiyveOBdYPVxd5At85ghYRNwB6SH_r5PQD1sZjddumx/pub?embedded=true">
</iframe>



<a id="organizers" class="anchor" href="#organizers" aria-hidden="true"><span class="octicon octicon-link"></span></a> 
<h2>Organizing Committee</h2>
<hr/>

<table width="80%" border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center"><img class="image1" src="/images/parisa.png" width="300" height="300" /> </td>
    <td align="center"><img class="image1" src="/images/xin3.jpeg" width="300" height="300" /> </td>
    <td align="center"><img class="image1" src="/images/yuezhang.jpg" width="300" height="300" /></td>
    <td align="center"><img class="image1" src="/images/martin.jpg" width="300" height="300" /> </td>
    <td align="center"><img class="image1" src="/images/mert.jpg" width="300" height="300" /> </td>
  </tr>
  <tr>
    <td align="center"><a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a><br>Michigan State University</br> kordjams@msu.edu</td>
    <td align="center"><a href="https://eric-xw.github.io/">Xin Eric Wang</a><br>University of California Santa Cruz</br>xwang366@ucsc.edu</td>
    <td align="center"><a href="https://www.egr.msu.edu/~zhan1624/">Yue Zhang</a><br>Michigan State University</br>zhan1624@msu.edu</td>
    <td align="center"><a href="https://mars-tin.github.io/">Ziqiao Ma</a><br>University of Michigan</br>marstin@umich.edu</td>
    <td align="center"><a href="https://merterm.github.io/">Mert Inan</a><br>Northeastern University</br>inan.m@northeastern.edu</td>
  </tr>
</table>

<h2>Advisory Committee</h2><hr/>

<table width="80%" border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center"><img class="image1" src="/images/ray.jpg" width="300" height="300" /> </td>
    <td align="center"><img class="image1" src="/images/joycechai.jpg" width="300" height="300" /> </td>
    <td align="center"><img class="image1" src="/images/tony.jpg" width="300" height="300" /> </td>
  </tr>
  <tr>
    <td align="center"><a href="https://www.cs.utexas.edu/~mooney/">Raymond J. Mooney</a><br>The University of Texas at Austin</br></td>
    <td align="center"><a href="https://web.eecs.umich.edu/~chaijy/">Joyce Y. Chai</a><br>University of Michigan</br></td>
    <td align="center"><a href="https://eps.leeds.ac.uk/computing/staff/76/professor-anthony-tony-g-cohn-freng-ceng-citp">Anthony G. Cohn</a><br>University of Leeds</br></td>
  </tr>
</table>
  
<a id="program-commitee" class="anchor" href="#program-commitee" aria-hidden="true"><span class="octicon octicon-link"></span></a>
<h2>Program Committee (Alphabetical Order)</h2><hr/>

<table cellspacing="0" cellpadding="0" style="width:100%">
	
  <tr>
  <td><li>Chris Paxton</li></td>
  <td>Meta</td>
  </tr>

  <tr>
    <td><li>Cristian-Paul Bara</li></td>
    <td>Bosch</td>
  </tr>

  <tr>
  <td><li>Felix Gervits</li></td>
  <td>Army Research Lab</td>
  </tr>
	
  <tr>
  <td><li>Drew A. Hudson</li></td>
  <td>Google DeepMind</td>
  </tr>
	
  <tr>
  <td><li>Jacob Krantz</li></td>
  <td>Oregon State University</td>
  </tr>
	
  <tr>
  <td><li>Jiachen Li</li></td>
  <td>University of California Santa Barbara</td>
  </tr>

  <tr>
  <td><li>Jiafei Duan</li></td>
  <td>University of Washington</td>
  </tr>
	
  <tr>
    <td><li>Jialu Li</li></td>
    <td>University of North Carolina at Chapel Hill</td>
  </tr>
	
  <tr>
  <td><li>Jiayi Pan</li></td>
  <td>University of California Berkeley</td>
  </tr>
	
  <tr>
  <td><li>Johan Bos</li></td>
  <td>University of Groningen</td>
  </tr>
	
  <tr>
  <td><li>Kirk Roberts</li></td>
  <td>University of Texas, Health Science Center at Houston</td>
  </tr>
	
  <tr>
  <td><li>Manling Li</li></td>
  <td>Northwestern University</td>
  </tr>
	
  <tr>
  <td><li>Natalie Parde</li></td>
  <td>University of Illinois Chicago</td>
  </tr>

  <tr>
    <td><li>Raphael Schumann</li></td>
    <td>Heidelberg University</td>
  </tr>
  
  <tr>
  <td><li>Roma Patel</li></td>
  <td>Google DeepMind</td>
  </tr>

  <tr>
  <td><li>Simon Dobnik</li></td>
  <td>University of Gothenburg</td>
  </tr>
	
  <tr>
  <td><li>Stephanie M. Lukin</li></td>
  <td>Army Research Lab</td>
  </tr>

  <tr>
  <td><li>Weiyu Liu</li></td>
  <td>Stanford University</td>
  </tr>
	
  <tr>
  <td><li>Xiaofeng Gao</li></td>
  <td>Amazon Alexa AI</td>
  </tr>
	
  <tr>
  <td><li>Yanyuan Qiao</li></td>
  <td>University of Adelaide</td>
  </tr>
	
  <tr>
  <td><li>Yichi Zhang</li></td>
  <td>University of Michigan</td>
  </tr>
	
  <tr>
  <td><li>Yue Fan</li></td>
  <td>University of California Santa Cruz</td>
  </tr>
</table>

<div>
<table cellspacing="0" cellpadding="0" float="left">
</table>
</div>

<h4>Ethics Committee</h4><hr/>
<table cellspacing="0" cellpadding="0" style="width:100%">

  <tr>
  <td><li>Luke Edward Richards</li></td>
  <td>University of Maryland Baltimore County</td>
  </tr>
	
  <tr>
  <td><li>Yuwei Bao</li></td>
  <td>University of Michigan</td>
  </tr>
	
</table>	

If you are interested in joining the program committee and participating in reviewing submissions,
 please email at splu-robonlp2024@googlegroups.com including your prior reviewing experience and a link to your publication records in your email.

</section>

<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11083511;
var sc_invisible=1;
var sc_security="2f97c6cf";
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="web analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11083511/0/2f97c6cf/1/" alt="web
analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
</body>
</html>
